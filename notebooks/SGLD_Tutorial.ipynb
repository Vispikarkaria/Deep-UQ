{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SGLD Sampling Tutorial\n",
        "Collect posterior samples with stochastic-gradient Langevin dynamics and aggregate predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal\n",
        "Perform stochastic-gradient Langevin dynamics (SGLD) to draw approximate posterior samples and aggregate predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Configure Python path so the notebook sees the local deepuq package\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path(os.getcwd())\n",
        "if not (PROJECT_ROOT / 'src').exists():\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "\n",
        "SRC_PATH = str(PROJECT_ROOT / 'src')\n",
        "if SRC_PATH not in sys.path:\n",
        "    sys.path.insert(0, SRC_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from deepuq.models import MLP\n",
        "from deepuq.methods import collect_posterior_samples, predict_with_samples\n",
        "from deepuq.utils import set_seed\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Running on {DEVICE}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n",
        "We again rely on MNIST; adjust the loader to your own dataset if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "set_seed(99)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))\n",
        "])\n",
        "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True)\n",
        "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Initialisation\n",
        "SGLD starts from random weights, then injects Gaussian noise during each update to explore the posterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "model = MLP(28*28, [128], 10, p_drop=0.0)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collect Posterior Samples\n",
        "`collect_posterior_samples` performs a short SGLD run. Increase `n_steps` for more thorough exploration; remember to adjust `burn_in` proportion accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "samples = collect_posterior_samples(\n",
        "    model=model,\n",
        "    data_loader=train_loader,\n",
        "    n_steps=50,        # total optimiser steps (including burn-in)\n",
        "    lr=1e-4,           # small step size keeps noise-controlled moves\n",
        "    weight_decay=1e-4, # weight decay acts as a Gaussian prior on weights\n",
        "    burn_in=0.4,       # discard first 40% of steps to reach steady state\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "print(f'Collected {len(samples)} posterior samples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predict with Sampled Weights\n",
        "Load each sampled parameter dictionary into the model, perform a forward pass, and aggregate the results to estimate mean and variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "with torch.inference_mode():\n",
        "    batch, _ = next(iter(test_loader))\n",
        "    batch = batch.to(DEVICE)\n",
        "    mean_probs, var_probs = predict_with_samples(model, samples, batch, apply_softmax=True, device=DEVICE)\n",
        "\n",
        "print('Predictive mean shape:', mean_probs.shape)\n",
        "print('Predictive variance shape:', var_probs.shape)\n",
        "print('First example mean probs:', mean_probs[0])\n",
        "print('First example variance:', var_probs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Advice\n",
        "- SGLD requires longer runs for well-mixed samples; monitor training loss and sample autocorrelation.\n",
        "- Store samples to disk if you need to pause and resume experiments.\n",
        "- Combine with thinning (keep every k-th sample) to reduce correlation between stored states."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}