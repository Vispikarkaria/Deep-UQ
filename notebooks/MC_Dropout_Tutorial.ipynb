{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Monte Carlo Dropout Tutorial\n",
        "Hands-on walk-through of uncertainty estimation using stochastic dropout at inference time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal\n",
        "Learn how to wrap a standard neural network with Monte Carlo Dropout to obtain predictive uncertainty measures. We'll train a small multilayer perceptron on MNIST for only a handful of mini-batches to keep runtime short."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "Make sure `torch`, `torchvision`, and `deepuq` are installed. Execute `pip install -e .` from the repository root to install the package in editable mode.\n",
        "> Tip: Run cells sequentially; inline comments describe the reasoning behind each operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Configure Python path so the notebook sees the local deepuq package\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path(os.getcwd())\n",
        "if not (PROJECT_ROOT / 'src').exists():\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "\n",
        "SRC_PATH = str(PROJECT_ROOT / 'src')\n",
        "if SRC_PATH not in sys.path:\n",
        "    sys.path.insert(0, SRC_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Core scientific stack\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# deepuq provides reusable architectures and uncertainty wrappers\n",
        "from deepuq.models import MLP\n",
        "from deepuq.methods import MCDropoutWrapper\n",
        "\n",
        "# Utility to keep experiments reproducible across runs\n",
        "from deepuq.utils import set_seed\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Running on {DEVICE}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Pipeline\n",
        "We'll use MNIST for familiarity. The transformation flattens each 28\u00d728 image into a 784-dimensional vector so it fits the dense MLP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "set_seed(42)  # lock in deterministic initialisation and data order\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # converts PIL images to [0,1] tensors\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # flatten so the MLP can ingest the vector\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# For notebook speed, keep loaders light. Adjust batch sizes as your hardware allows.\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=256, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the Deterministic Network\n",
        "The `MLP` from `deepuq.models` stacks linear layers, ReLU activations, and dropout. Dropout probability is the lever that controls predictive spread when we enable MC sampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "model = MLP(input_dim=28*28, hidden_dims=[512, 256], output_dim=10, p_drop=0.2)\n",
        "model.to(DEVICE)\n",
        "\n",
        "# Adam usually converges quickly on MNIST; feel free to experiment with SGD or different learning rates.\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Training Loop\n",
        "We only iterate over a handful of mini-batches to illustrate the workflow. For serious experiments, increase the number of epochs and monitor validation metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "model.train()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_batches = 5  # keep training snappy inside the notebook\n",
        "for step, (features, labels) in enumerate(train_loader, start=1):\n",
        "    features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(features)\n",
        "    loss = loss_fn(logits, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Batch {step}/{num_batches} - training loss: {loss.item():.3f}')\n",
        "    if step >= num_batches:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wrap with Monte Carlo Dropout\n",
        "`MCDropoutWrapper` keeps dropout layers active during evaluation and runs repeated stochastic forward passes to build a predictive distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "model.eval()  # base model enters eval mode before wrapping\n",
        "uq_model = MCDropoutWrapper(model=model, n_mc=50, apply_softmax=True)\n",
        "uq_model.to(DEVICE)\n",
        "\n",
        "# Grab a batch of test images to interrogate uncertainty.\n",
        "sample_batch, sample_labels = next(iter(test_loader))\n",
        "sample_batch = sample_batch.to(DEVICE)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    mean_probs, var_probs = uq_model.predict(sample_batch)\n",
        "\n",
        "print('Predictive mean shape:', mean_probs.shape)\n",
        "print('Predictive variance shape:', var_probs.shape)\n",
        "print('Example mean probs for first sample:', mean_probs[0])\n",
        "print('Example predictive variance for first sample:', var_probs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpreting the Numbers\n",
        "- `mean_probs`: average class probabilities over 50 stochastic forward passes.\n",
        "- `var_probs`: per-class variance capturing epistemic uncertainty from dropout randomness.\n",
        "You can visualise variance scores or aggregate them (e.g., maximum variance) to flag uncertain predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "1. Increase `n_mc` to tighten Monte Carlo estimates at the cost of runtime.\n",
        "2. Calibrate probabilities with temperature scaling using a held-out validation set.\n",
        "3. Swap in your own architecture\u2014`MCDropoutWrapper` works with any `nn.Module` containing dropout layers."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}